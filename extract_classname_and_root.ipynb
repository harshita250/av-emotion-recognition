{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "225d324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d808737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roots=[]\n",
    "meta_data = {'class_names':'','class_roots':''}\n",
    "test_path = r'C:\\Users\\harsh\\Downloads\\images_background\\images_background'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f5f5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filenames(path):\n",
    "    for name in os.listdir(test_path):\n",
    "        roots.append((os.path.join(test_path,name)))\n",
    "    meta_data['class_names'] = os.listdir(path)\n",
    "    meta_data['class_roots'] = roots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ce33f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_filenames(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "efbeb29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_names': ['Alphabet_of_the_Magi',\n",
       "  'Anglo-Saxon_Futhorc',\n",
       "  'Arcadian',\n",
       "  'Armenian',\n",
       "  'Asomtavruli_(Georgian)',\n",
       "  'Balinese',\n",
       "  'Bengali',\n",
       "  'Blackfoot_(Canadian_Aboriginal_Syllabics)',\n",
       "  'Braille',\n",
       "  'Burmese_(Myanmar)',\n",
       "  'Cyrillic',\n",
       "  'Early_Aramaic',\n",
       "  'Futurama',\n",
       "  'Grantha',\n",
       "  'Greek',\n",
       "  'Gujarati',\n",
       "  'Hebrew',\n",
       "  'Inuktitut_(Canadian_Aboriginal_Syllabics)',\n",
       "  'Japanese_(hiragana)',\n",
       "  'Japanese_(katakana)',\n",
       "  'Korean',\n",
       "  'Latin',\n",
       "  'Malay_(Jawi_-_Arabic)',\n",
       "  'Mkhedruli_(Georgian)',\n",
       "  'N_Ko',\n",
       "  'Ojibwe_(Canadian_Aboriginal_Syllabics)',\n",
       "  'Sanskrit',\n",
       "  'Syriac_(Estrangelo)',\n",
       "  'Tagalog',\n",
       "  'Tifinagh'],\n",
       " 'class_roots': ['C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Alphabet_of_the_Magi',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Anglo-Saxon_Futhorc',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Arcadian',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Armenian',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Asomtavruli_(Georgian)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Balinese',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Bengali',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Blackfoot_(Canadian_Aboriginal_Syllabics)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Braille',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Burmese_(Myanmar)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Cyrillic',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Early_Aramaic',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Futurama',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Grantha',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Greek',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Gujarati',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Hebrew',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Inuktitut_(Canadian_Aboriginal_Syllabics)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Japanese_(hiragana)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Japanese_(katakana)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Korean',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Latin',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Malay_(Jawi_-_Arabic)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Mkhedruli_(Georgian)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\N_Ko',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Ojibwe_(Canadian_Aboriginal_Syllabics)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Sanskrit',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Syriac_(Estrangelo)',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Tagalog',\n",
       "  'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_background\\\\images_background\\\\Tifinagh']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3bc08cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_details = {'filename':{'audio':'','frames':[]}}\n",
    "#meta_data[0]['filename']['audio']\n",
    "file_details={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshotdata(path):\n",
    "    for key,value in file_details.items():\n",
    "        support_set = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d95c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "from easyfsl.datasets import FewShotDataset\n",
    "\n",
    "\n",
    "class TaskSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples batches in the shape of few-shot classification tasks. At each iteration, it will sample\n",
    "    n_way classes, and then sample support and query images from these classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: FewShotDataset,\n",
    "        n_way: int,\n",
    "        n_shot: int,\n",
    "        n_query: int,\n",
    "        n_tasks: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: dataset from which to sample classification tasks. Must have a field 'label': a\n",
    "                list of length len(dataset) containing containing the labels of all images.\n",
    "            n_way: number of classes in one task\n",
    "            n_shot: number of support images for each class in one task\n",
    "            n_query: number of query images for each class in one task\n",
    "            n_tasks: number of tasks to sample\n",
    "        \"\"\"\n",
    "        super().__init__(data_source=None)\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_tasks = n_tasks\n",
    "\n",
    "        self.items_per_label = {}\n",
    "        for item, label in enumerate(dataset.get_labels()):\n",
    "            if label in self.items_per_label.keys():\n",
    "                self.items_per_label[label].append(item)\n",
    "            else:\n",
    "                self.items_per_label[label] = [item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_tasks\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_tasks):\n",
    "            yield torch.cat(\n",
    "                [\n",
    "                    # pylint: disable=not-callable\n",
    "                    torch.tensor(\n",
    "                        random.sample(\n",
    "                            self.items_per_label[label], self.n_shot + self.n_query\n",
    "                        )\n",
    "                    )\n",
    "                    # pylint: enable=not-callable\n",
    "                    for label in random.sample(self.items_per_label.keys(), self.n_way)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def episodic_collate_fn(\n",
    "        self, input_data: List[Tuple[Tensor, int]]\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, List[int]]:\n",
    "        \"\"\"\n",
    "        Collate function to be used as argument for the collate_fn parameter of episodic\n",
    "            data loaders.\n",
    "        Args:\n",
    "            input_data: each element is a tuple containing:\n",
    "                - an image as a torch Tensor\n",
    "                - the label of this image\n",
    "        Returns:\n",
    "            tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
    "                - support images,\n",
    "                - their labels,\n",
    "                - query images,\n",
    "                - their labels,\n",
    "                - the dataset class ids of the class sampled in the episode\n",
    "        \"\"\"\n",
    "\n",
    "        true_class_ids = list({x[1] for x in input_data})\n",
    "\n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data])\n",
    "        all_images = all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        )\n",
    "        # pylint: disable=not-callable\n",
    "        all_labels = torch.tensor(\n",
    "            [true_class_ids.index(x[1]) for x in input_data]\n",
    "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
    "        # pylint: enable=not-callable\n",
    "\n",
    "        support_images = all_images[:, : self.n_shot].reshape(\n",
    "            (-1, *all_images.shape[2:])\n",
    "        )\n",
    "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
    "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
    "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
    "\n",
    "        return (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            true_class_ids,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af48398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Set, Tuple, Callable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from easyfsl.datasets import FewShotDataset\n",
    "from easyfsl.datasets.default_configs import default_transform, DEFAULT_IMAGE_FORMATS\n",
    "\n",
    "\n",
    "class EasySet(FewShotDataset):\n",
    "    \"\"\"\n",
    "    A ready-to-use dataset. Will work for any dataset where the images are\n",
    "    grouped in directories by class. It expects a JSON file defining the\n",
    "    classes and where to find them. It must have the following shape:\n",
    "        {\n",
    "            \"class_names\": [\n",
    "                \"class_1\",\n",
    "                \"class_2\"\n",
    "            ],\n",
    "            \"class_roots\": [\n",
    "                \"path/to/class_1_folder\",\n",
    "                \"path/to/class_2_folder\"\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        specs_file: Union[Path, str],\n",
    "        image_size: int = 84,\n",
    "        transform: Callable = None,\n",
    "        training: bool = False,\n",
    "        supported_formats: Set[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            specs_file: path to the JSON file\n",
    "            image_size: images returned by the dataset will be square images of the given size\n",
    "            transform: torchvision transforms to be applied to images. If none is provided,\n",
    "                we use some standard transformations including ImageNet normalization.\n",
    "                These default transformations depend on the \"training\" argument.\n",
    "            training: preprocessing is slightly different for a training set, adding a random\n",
    "                cropping and a random horizontal flip. Only used if transforms = None.\n",
    "            supported_formats: set of allowed file format. When listing data instances, EasySet\n",
    "                will only consider these files. If none is provided, we use the default set of\n",
    "                image formats.\n",
    "        \"\"\"\n",
    "        specs = self.load_specs(Path(specs_file))\n",
    "\n",
    "        self.images, self.labels = self.list_data_instances(\n",
    "            specs[\"class_roots\"], supported_formats=supported_formats\n",
    "        )\n",
    "\n",
    "        self.class_names = specs[\"class_names\"]\n",
    "\n",
    "        self.transform = (\n",
    "            transform if transform else default_transform(image_size, training)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def load_specs(specs_file: Path) -> dict:\n",
    "        \"\"\"\n",
    "        Load specs from a JSON file.\n",
    "        Args:\n",
    "            specs_file: path to the JSON file\n",
    "        Returns:\n",
    "            dictionary contained in the JSON file\n",
    "        Raises:\n",
    "            ValueError: if specs_file is not a JSON, or if it is a JSON and the content is not\n",
    "                of the expected shape.\n",
    "        \"\"\"\n",
    "\n",
    "        if specs_file.suffix != \".json\":\n",
    "            raise ValueError(\"EasySet requires specs in a JSON file.\")\n",
    "\n",
    "        with open(specs_file, \"r\") as file:\n",
    "            specs = json.load(file)\n",
    "\n",
    "        if \"class_names\" not in specs.keys() or \"class_roots\" not in specs.keys():\n",
    "            raise ValueError(\n",
    "                \"EasySet requires specs in a JSON file with the keys class_names and class_roots.\"\n",
    "            )\n",
    "\n",
    "        if len(specs[\"class_names\"]) != len(specs[\"class_roots\"]):\n",
    "            raise ValueError(\n",
    "                \"Number of class names does not match the number of class root directories.\"\n",
    "            )\n",
    "\n",
    "        return specs\n",
    "\n",
    "    @staticmethod\n",
    "    def list_data_instances(\n",
    "        class_roots: List[str], supported_formats: Set[str] = None\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Explore the directories specified in class_roots to find all data instances.\n",
    "        Args:\n",
    "            class_roots: each element is the path to the directory containing the elements\n",
    "                of one class\n",
    "        Returns:\n",
    "            list of paths to the images, and a list of same length containing the integer label\n",
    "                of each image\n",
    "        \"\"\"\n",
    "        if supported_formats is None:\n",
    "            supported_formats = DEFAULT_IMAGE_FORMATS\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "        for class_id, class_root in enumerate(class_roots):\n",
    "            class_images = [\n",
    "                str(image_path)\n",
    "                for image_path in sorted(Path(class_root).glob(\"*\"))\n",
    "                if image_path.is_file() & (image_path.suffix in supported_formats)\n",
    "            ]\n",
    "            images += class_images\n",
    "            labels += len(class_images) * [class_id]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        \"\"\"\n",
    "        Get a data sample from its integer id.\n",
    "        Args:\n",
    "            item: sample's integer id\n",
    "        Returns:\n",
    "            data sample in the form of a tuple (image, label), where label is an integer.\n",
    "            The type of the image object depends of the output type of self.transform. By default\n",
    "            it's a torch.Tensor, however you are free to define any function as self.transform, and\n",
    "            therefore any type for the output image. For instance, if self.transform = lambda x: x,\n",
    "            then the output image will be of type PIL.Image.Image.\n",
    "        \"\"\"\n",
    "        # Some images of ILSVRC2015 are grayscale, so we convert everything to RGB for consistence.\n",
    "        # If you want to work on grayscale images, use torch.transforms.Grayscale in your\n",
    "        # transformation pipeline.\n",
    "        img = self.transform(Image.open(self.images[item]).convert(\"RGB\"))\n",
    "        label = self.labels[item]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_labels(self) -> List[int]:\n",
    "        return self.labels\n",
    "\n",
    "    def number_of_classes(self):\n",
    "        return len(self.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e792311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(test_path):\n",
    "    for name in os.listdir(test_path):\n",
    "        file_path = os.path.join(test_path,name)\n",
    "        for x,y,files in os.walk(file_path):\n",
    "            frames = [file for file in files if file[-3:] == 'jpg']\n",
    "            audio = [file for file in files if file[-3:] == 'wav']\n",
    "            file_details[file_path]={'audio':audio,'frames':frames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16af6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easyfsl\n",
      "  Downloading easyfsl-1.0.0-py3-none-any.whl (49 kB)\n",
      "     -------------------------------------- 49.2/49.2 KB 418.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: torchvision>=0.7.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from easyfsl) (0.10.0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from easyfsl) (1.2.4)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from easyfsl) (3.3.4)\n",
      "Requirement already satisfied: tqdm>=4.1.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from easyfsl) (4.59.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from easyfsl) (1.9.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->easyfsl) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas>=1.1.0->easyfsl) (2021.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from torch>=1.4.0->easyfsl) (3.7.4.3)\n",
      "Requirement already satisfied: six in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->easyfsl) (1.16.0)\n",
      "Installing collected packages: easyfsl\n",
      "Successfully installed easyfsl-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\harsh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\harsh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\harsh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\harsh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\harsh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\harsh\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install easyfsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1729292a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_evaluation\\\\images_evaluation\\\\Alphabet_of_the_Magi': {'audio': [],\n",
       "  'frames': []},\n",
       " 'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_evaluation\\\\images_evaluation\\\\Angelic': {'audio': [],\n",
       "  'frames': []},\n",
       " 'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_evaluation\\\\images_evaluation\\\\Anglo-Saxon_Futhorc': {'audio': [],\n",
       "  'frames': []},\n",
       " 'C:\\\\Users\\\\harsh\\\\Downloads\\\\images_evaluation\\\\images_evaluation\\\\Arcadian': {'audio': [],\n",
       "  'frames': []}}"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_metadata(test_path)\n",
    "file_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "e6e5b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\Downloads\\images_evaluation\\images_evaluation\\Alphabet_of_the_Magi : {'audio': [], 'frames': []}\n",
      "C:\\Users\\harsh\\Downloads\\images_evaluation\\images_evaluation\\Angelic : {'audio': [], 'frames': []}\n",
      "C:\\Users\\harsh\\Downloads\\images_evaluation\\images_evaluation\\Anglo-Saxon_Futhorc : {'audio': [], 'frames': []}\n",
      "C:\\Users\\harsh\\Downloads\\images_evaluation\\images_evaluation\\Arcadian : {'audio': [], 'frames': []}\n"
     ]
    }
   ],
   "source": [
    "for key,value in file_details.items():\n",
    "    print(key[],':',value)\n",
    "    #print(key,':',value['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d091d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r'C:\\Users\\harsh\\Downloads\\images_evaluation\\images_evaluation'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
